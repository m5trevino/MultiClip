hi everyone welcome back to the AI language today we are going to make a lang chain based mCP client which supports a Json config for multiple servers and so we can actually provide a file like the AI language unor config.js which can actually specify multiple servers similar to what you have for CLA desktop but this time we're going to make it on our own using Lang chain or Lang graph for the react a agent and we are going to use mCP and Google Gemini API for the large language model and we are going to do this in Python if you want to learn more about mCP please head over to our channel the AI language on YouTube and subscribe to it and we have and we have an end to-end model context protocol course which is live and being updated every few days with the latest developments on mCP and you can find several videos over here for the course so let me just show you the sample config.js file that I'll use over here we'll have two mCP servers defined one will be the terminal server that we have previously built in this course and the other will be a pre-existing mCP server which for now I'm using as fetch you can choose any one of the mCP servers and I'm going to actually use the docker command to run both these servers and we've already looked at how to build a a terminal server and put this in a Docker container and then use it with your own client now before I start let me show you how this works and for that I'll head over to the terminal so let me change to our mCP directory then to clients then to mCP client then let me run our Lang chain mCP client with config.py this is going to start our mCP client and as you can see it basically connects to the mCP server terminal server which is our first mCP server and it loads the tool run command and this basically allows the mCP client and LM to run commands on the terminal on the Local Host machine and then it says Ive loaded one tool from terminal server then it connects to mCP server fetch this is the pre-existing reference server from the model context protocol GitHub and it loads the tool fetch which can basically fetch content from a URL and then it says I loaded one tool from Fetch and then it says the mCP client is ready and then type quit to exit so let's ask the client first what all tools are available to it all right so what tools do you have available to use all right so to this query basically the response is that I have access to the following tools run command to run terminal commands and fetch to fetch content from a URL now let's provide a query to fetch content from a Wiki URL and then summarize it and write it to a file using a terminal command all right so we'll write the query can you fetch content from the Wiki page for airplanes first few lines and write a summary of the content to a file called airplanes. txt let's press enter and see what happens all right so I've explicitly specified the Run command tool over here since Gemini seems to not recognize this while writing the content okay guys so now this is the response that I've have got so there's the message that we sent and uh then we have the tool message which says contents of the Wikipedia page on airplanes and then it says that you know you can actually call the fetch tool with a start index of 500 to get more content and then it does that it says that the content was truncated I will I will fetch more content to provide a summary and this is more content and then again uh there's a new start index and then there's again uh more content and then seems there was a connection issue and uh it could not fetch more so it says I was unable to fetch the entire content of the Wikipedia page do truncation and connection issues however I have the first few lines and here's a summary first uh for the first content and then at the end it says that uh I will use the Run command tool to save this summary and then I freshed the first few lines from Wikipedia page for airplanes and created a summary of the content and I've written this to a file called airplanes. txt using the Run command tool so let's go to the finder and check this file out so here's the file airplanes. txt in our workspace directory and if we double click to open this we can see that there's a summary of content whatever it was able to fetch over here so an aeroplane is a fixed Wing aircraft propelled by a jet engine propeller or rocket engine they come in various sizes shapes and Wing configurations and I used for recreation Transportation military and research and and so on so it seems to be that it has actually written a summary of whatever content it could fetch from the Wikipedia page and return it to a file using both the Fetch and the Run command tool so this is pretty amazing now we have our own mCP client which basically works with the Google Gemini API and and it can actually connect and initialize servers based on a config.js file just like CLA desktop and then use the tools to perform actions using the react agent from langra and this makes a client more robust with this added functionality all right so now let's walk through the code for this client with the config you can check this code out from the GitHub repository by logging into the AI language.com and the link is in the description and the pinned comment all right so the name of the file for this client is Lang chain mCP client WC config.py and wc config basically stands for with config so this code implements a lang chain mCP client that loads a configuration from a Json file specified by the AI language config environment variable and connects to one or more mCP servers defined in this config it loads the available mCP tools from each connected server and then uses the Google Gemini API via Lang chain to create a react agent with access to all the tools and then it runs an interactive chat Loop where user queries are processed by the agent now this client builds up upon the previous client that we built on Lang chain mCP client.py so in case you've not gone through this particular code please do that by looking at the previous video it's available on our model context protocol course on our YouTube channel and if you scroll down it's this video number five of building a lang chain mCP client with python and Google the other thing that we'll be using from our previous video is the docker mCP server which is the server that can run commands on our terminal and we call as terminal server this is covered in video number six build Docker mCP server and I'm going to use this particular server as one of the servers in this particular demo so we're building upon this and I'm going to cover the differences between this client and the one that we have built right now all right so first thing is that this client uses the AI language config environment variable to specify the path for the AI language config.js file if that path is not set it uses the default file available in the code root directory the AI language config.js now if you scroll down we have the regular Imports that we have we have our custom encoder then we now Define a read config Json function and this basically Ally reads the mCP server config Json and again it tries to read the path from the AI language config environment variable and if not self it falls back to a default file the AI language config.js in the same directory as the code and it returns a dictionary pass Json content with mCP server definitions this is the attempt to read the environment variable and then if this is not set there's a fall back to get the script directory and then join the file name with the script directory to get the actual file path then it opens the path whichever it gets either the environment variable or the default path and it Returns the Json that it's loaded from there and we handle exceptions to print an error message and exit then we instantiate the Google Gemini llm so one difference over here is that we are using Gemini 2.0 Flash and there's a reason for this the reason is that you can actually go to AI Studio . google.com and login with your Gmail account and then you can check out the different models over here so Gemini 2.0 flash has a request per minute rate limit for 15 requests per minute if you are to look at some of the Pro Models like Gemini 2.5 Pro experimental it has just five requests per minute now this is also the case with 1.5 Pro which has two requests per minute this is too low for an agent execution which might need multiple calls per minute but we don't don't expect to go over 15 requests per minute which is provided by Gemini 2.0 flash so we are good with this particular model selection so that's one difference from the previous client and then we have the Google API key which we load so now we have the Run agent function which connects to all the mCP servers defined in the configuration loads their tools and creates a unified react agent from langra and then starts an interactive Loop to query the agent this is unlike the previous client implementation where we just had one server that we were connecting to through a python script so we basically first read the config then we basically get the mCP server definitions from the config and if there are no mCP servers inside it then we basically print an error message and exit we then Define and initialize an empty list to hold all the tools from the connected servers because we want to unify them into one single list and now basically using this async exit stack to basically manage and cleanly close multiple async resources we iterate over each mCP server defined in the configuration so now we basically have the server name and the server info in mCP servers. item so we have loaded the mCP servers from the config file and let's look at the config file once again to understand this so mCP servers has these items item number one and item number two and each item has a server name and then its information in terms of command and arguments so now basically you get the server name which basically corresponds to either terminal server or fetch and you get the server info which basically corresponds to all the information that we have noted against it the command and the arguments we then print that we're connecting to mCP server with the specific name and then we create the stdio server parameters using the command and argument specified for the server so we'll specify the command which in this case is going to be Docker for both and then we specify the arguments like so and for Docker we basically use the docker image file over here so this basically gives us server params which we are then going to use to establish an stdio connection to the server so we use the server parameter and then we get the read and write stream objects for this particular server so we are looping through each server so for one server at a time we do all these steps we then use the read and write parameters to create a client session and then we initialize the session and then we load the mCP tools into server tools and for each server tool we'll basically inform the user that we have loaded the tool with a specific name and then we append it to our list of unified tools once we are done with all the tools for this particular server we'll say that we have basically loaded tools from this particular servers and we'll use the length of server tools to denote how many tools were loaded then the for Loop continues for the next server and this way we load all the tools into a single unified tools list that we then going to pass to the agent all right so then we have some basic error handling and after that we basically create the agent which is a react agent from langra and we provided the llm which is the Google Gemini llm instantiation that we did above along with the unified list of all the tools then we start an interactive chat Loop like we did for our previous mCP client and this expects a quit statement to quit the loop otherwise it basically invokes the agent with the query provided by the user and then prints the response and then of course we have the entry point over here all right so that's all about the code for the Lang chain mCP client with the Json config file now let's look at the config file itself now you would remember terminal server from the previous videos in this course and we have used this previously with the Lang chain client without the config now we actually add more servers to this and for this I have used a pre-existing server just to check out my implementation of the mCP client with the config for this you can choose any specific server you want what I have done is I have searched for the mCP servers list and then gone to the GitHub page and over here you have several mCP servers I have chosen the fetch server from here which basically can fetch web content because what I aimed at making as a demo was to fetch content summarize it and then write it to a file using my own terminal server so if you click on the fetch server you can see that they have a Docker file and they provide a configuration to add to CLA using Docker where the command Docker is used and then they have the Run command with the image name so when setting this up what you need to do is you need to first clone these servers so we head back to the main GitHub page for all the servers and then we actually copy the Clone command I have SSH setup so I'm going to copy the SSH clone command from here you can open a fresh terminal window and you can go to CD mCP and over here you can actually make a directory for these reference servers so you can actually clone the servers into a directory called servers ref I've already done that and these are basically all the pre-existing reference servers provided on their GitHub page to do this just type get clone and then paste what you copied from there and provide a name for the directory which is servers unor ref and then press enter I've already done this so I'm not going to repeat it and you can clone the directory like this to this particular folder note that we are doing this under our mCP directory and this is going to make the servers ref directory inside it you don't need to create it separately once you have this created you can actually shift to the server's ref directory and then you'll find a SRC directory which is a source directory inside it and then you'll find the f directory inside it now I'm inside the fet directory and you can do an ls- L to list all the files over here and you see that you have the docker file over here so before we can use this particular mCP server in our mCP client config file we need to build the docker image so for that we can use the command Docker build DT which basically tags the image with a specific name you can provided with some name so let me just check the name that I have used I've used mCP fet server test so I'm going to just copy it to demonstrate how you can build it and this is how you build it and then you put a dot at the end to basically provide the current directory as build context and press enter and this is basically going to build the docker image for you and now your Docker imag is built and then you can actually go to the AI language config.js file and when when you specify this Docker image name dogger understands that this is what you need to actually leun of course if you know from a previous videos you need to have Docker desktop running in the background for which you can actually open Docker desktop by going to your applications folder and looking for Docker over there okay so with this config.js file and with the Lang chain mCP client with config.py file we actually have our mCP client set up and and now we can basically head over to the terminal to actually run this particular client and test it out thank you for watching don't forget to check out the mCP course which is an endtoend model context protocol course on our YouTube channel the AI language if you have any questions or queries please feel free to write them out in the comments and I'll try my best to answer them as soon as possible thank you for watching and I'll see you in the next video